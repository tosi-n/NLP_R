library(gapminder)
library(gganimate)
## standard ggplot2
ggplot(oloni_locations, aes(x = lng, y = lat, size = pop, colour = country)) +
geom_point(alpha = 0.7, show.legend = FALSE) +
scale_colour_manual(values = country_colors) +
scale_size(range = c(2, 12)) +
scale_x_log10() +
# Here comes the gganimate specific bits
labs(title = 'Tweet Locations For Oloni Thread Explosion', x = 'Longitude', y = 'Latitude') +
transition_time(minute) +
ease_aes('linear')
install.packages("gifski")
library(gapminder)
library(gganimate)
library(gifski)
## standard ggplot2
ggplot(gapminder, aes(x = lng, y = lat, size = pop, colour = country)) +
geom_point(alpha = 0.7, show.legend = FALSE) +
scale_colour_manual(values = country_colors) +
scale_size(range = c(2, 12)) +
scale_x_log10() +
# Here comes the gganimate specific bits
labs(title = 'Tweet Locations For Oloni Thread Explosion', x = 'Longitude', y = 'Latitude') +
transition_time(minute) +
ease_aes('linear')
alc <- read.csv("ALC.csv", header = TRUE)
alc <- read.csv("ALC", header = TRUE)
alc <- read.csv("ALC", header = TRUE)
alc <- read.csv("ALC.csv", header = TRUE)
alc <- na.omit(alc)
alc
alc <- read.csv("ALC.csv", header = TRUE)
alc <- read.csv("ALC1.csv", header = TRUE)
head(alc)
library(gggplot2)
library(ggplot2)
ggplot(alc) +
aes(x = ICD.10.Description, y = All.persons)
world_basemap +
geom_point(data = oloni_locations, aes(x = lng, y = lat),
colour = 'purple', alpha = .5) +
scale_size_continuous(range = c(1, 8),
breaks = c(250, 500, 750, 1000)) +
labs(title = "Tweet Locations For Oloni Thread Explosion")
# load twitter library - the rtweet library is recommended now over twitteR
#install.packages("rtweet")
#install.packages("tidytext")
library(rtweet)
# plotting and pipes - tidyverse!
library(ggplot2)
library(dplyr)
# text mining library
library(tidytext)
# plotting packages
library(twitteR)
library(widyr)
library(tm)
library(igraph)
library(ggraph)
# json support
library(rjson)
library(jsonlite)
library(RCurl)
library(data.table)
library(leaflet)
library(RJSONIO)
library(gganimate)
library(lubridate)
library(maps)
library(ggthemes)
options(stringsAsFactors = FALSE)
#twitter API lib in R
## install dev version of rtweet from github
#devtools::install_github("mkearney/rtweet")
## web browser method: create token and save it as an environment variable
#create_token(
#  app = "TWRAP_test",
#  consumer_key = "RkAMdcjdqcjaGTNJtFGvQLWQT",
#  consumer_secret = "hgPm0HjCzjPBi3Bs50FpM3c2latGspNZxh6156Ct5dJhFdWtqE")
#credentials got from the twitter API page, at the twitter developers site.
## access token method: create token and save it as an environment variable
twitter_token <- create_token(
app = "TWRAP_test",
consumer_key <- "RkAMdcjdqcjaGTNJtFGvQLWQT",
consumer_secret<- "hgPm0HjCzjPBi3Bs50FpM3c2latGspNZxh6156Ct5dJhFdWtqE",
access_token <- "347721539-lRLGqB52mB9sPo2l3rUoSzvxsy5G9I4lNZXbQatx",
access_secret <- "zsyJQ21zTF0cgn0MeKS1L6e8LLQk5Le39J1ZSKuDRWJYS")
#set up authenticate
# choose yes for auto authentication or  no otherwise
setup_twitter_oauth(consumer_key ,consumer_secret,access_token ,access_secret)
# post a tweet from R
post_tweet("Back at it with R in my #rstats #earthanalytics session! @EarthLabCU,
Looking toward mapping and visualising animated tweets.
This should be fun. Tweeting from R is FUN.
I need to figure out how to add smileys from R")
rstats_tweets <- search_tweets(q = "#NigeriaDecides",
n = 500)
# view the first 3 rows of the dataframe
head(rstats_tweets, n = 3)
# find recent tweets with #rstats but ignore retweets
rstats_tweets <- search_tweets("#rstats", n = 500,
include_rts = FALSE)
# view top 2 rows of data
head(rstats_tweets, n = 2)
# view column with screen names - top 6
head(rstats_tweets$screen_name)
unique(rstats_tweets$screen_name)
# what users are tweeting with #rstats
users <- search_users("#rstats",
n = 500)
# just view the first 2 users - the data frame is large!
head(users, n = 2)
# how many locations are represented
length(unique(users$location))
## [1] 304
users %>%
ggplot(aes(location)) +
geom_bar() + coord_flip() +
labs(x = "Count",
y = "Location",
title = "Twitter users - unique locations ")
users %>%
count(location, sort = TRUE) %>%
mutate(location = reorder(location, n)) %>%
top_n(20) %>%
ggplot(aes(x = location, y = n)) +
geom_col() +
coord_flip() +
labs(x = "Count",
y = "Location",
title = "Where Twitter users are from - unique locations")
users %>%
count(location, sort = TRUE) %>%
mutate(location = reorder(location,n)) %>%
na.omit() %>%
top_n(20) %>%
ggplot(aes(x = location,y = n)) +
geom_col() +
coord_flip() +
labs(x = "Location",
y = "Count",
title = "Twitter users - unique locations ")
colnames(users)
climate_tweets <- search_tweets(q = "#climatechange", n = 10000,
lang = "en",
include_rts = FALSE)
# check data to see if there are emojis
head(climate_tweets$text)
# remove urls tidyverse is failing here for some reason
# climate_tweets %>%
#  mutate_at(c("stripped_text"), gsub("http.*","",.))
# remove http elements manually
climate_tweets$stripped_text <- gsub("http.*","",  climate_tweets$text)
climate_tweets$stripped_text <- gsub("https.*","", climate_tweets$stripped_text)
# note the words that are recognized as unique by R
a_list_of_words <- c("Dog", "dog", "dog", "cat", "cat", ",")
unique(a_list_of_words)
# remove punctuation, convert to lowercase, add id for each tweet!
climate_tweets_clean <- climate_tweets %>%
dplyr::select(stripped_text) %>%
unnest_tokens(word, stripped_text)
# plot the top 15 words -- notice any issues?
climate_tweets_clean %>%
count(word, sort = TRUE) %>%
top_n(15) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Count",
y = "Unique words",
title = "Count of unique words found in tweets")
# load list of stop words - from the tidytext package
data("stop_words")
# view first 6 words
head(stop_words)
nrow(climate_tweets_clean)
# remove stop words from your list of words
cleaned_tweet_words <- climate_tweets_clean %>%
anti_join(stop_words)
# there should be fewer words now
nrow(cleaned_tweet_words)
# plot the top 15 words -- notice any issues?
cleaned_tweet_words %>%
count(word, sort = TRUE) %>%
top_n(15) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(y = "Count",
x = "Unique words",
title = "Count of unique words found in tweets",
subtitle = "Stop words removed from the list")
library(devtools)
#install_github("dgrtwo/widyr")
library(widyr)
# remove punctuation, convert to lowercase, add id for each tweet!
climate_tweets_paired_words <- climate_tweets %>%
dplyr::select(stripped_text) %>%
unnest_tokens(paired_words, stripped_text, token = "ngrams", n = 2)
climate_tweets_paired_words %>%
count(paired_words, sort = TRUE)
#library(tidyr)
climate_tweets_separated_words <- climate_tweets_paired_words %>%
tidyr::separate(paired_words, c("word1", "word2"), sep = " ")
climate_tweets_filtered <- climate_tweets_separated_words %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
# new bigram counts:
climate_words_counts <- climate_tweets_filtered %>%
count(word1, word2, sort = TRUE)
head(climate_words_counts)
# plot climate change word network
climate_words_counts %>%
filter(n >= 24) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 3) +
geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
labs(title = "Word Network: Tweets using the hashtag - Climate Change",
subtitle = "Text mining twitter data ",
x = "", y = "")
# create file path
json_file <- "/Users/lade/Documents/Tosin_R_root/boulder_flood_geolocated_tweets.json"
# import json file line by line to avoid syntax errors
# this takes a few seconds
boulder_flood_tweets <- stream_in(file(json_file))
# Base URL path
base_url = "https://data.colorado.gov/resource/tv8u-hswn.json?"
full_url = paste0(base_url, "county=Boulder",
"&$where=age between 20 and 40",
"&$select=year,age,femalepopulation")
full_url
# encode the URL with characters for each space.
full_url <- URLencode(full_url)
full_url
# Convert JSON to data frame
pop_proj_data_df <- fromJSON(getURL(full_url))
head(pop_proj_data_df, n = 2)
typeof(pop_proj_data_df)
str(pop_proj_data_df)
# turn columns to numeric and remove NA values
pop_proj_data_df <- pop_proj_data_df %>%
mutate_at(c( "age", "year", "femalepopulation"), as.numeric)
# plot the data
ggplot(pop_proj_data_df, aes(x = year, y = femalepopulation,
group = factor(age), color = age)) + geom_line() +
labs(x = "Year",
y = "Female Population - Age 20-40",
title = "Projected Female Population",
subtitle = "Boulder, CO: 1990 - 2040")
# encode the URL with characters for each space.
full_url <- URLencode(full_url)
full_url
# Convert JSON to data frame
pop_proj_data_df <- fromJSON(getURL(full_url))
head(pop_proj_data_df, n = 2)
typeof(pop_proj_data_df)
str(pop_proj_data_df)
# turn columns to numeric and remove NA values
pop_proj_data_df <- pop_proj_data_df %>%
mutate_at(c( "age", "year", "femalepopulation"), as.numeric)
# turn columns to numeric and remove NA values
pop_proj_data_df <- pop_proj_data_df %>%
mutate_at(c( "age", "year", "femalepopulation"), as.numeric)
#Get the location of @UMassPoll. To run the following code, you must first set up Twitter API.
user<-getUser("Oloni")
user$location
## stream london tweets for a week (60 secs x 60 mins * 24 hours *  7 days)
stream_tweets(
c("oloni,Simplyoloni"),
timeout = 60 * 10,
file_name = "oloni.json",
parse = FALSE
)
## stream london tweets for a week (60 secs x 60 mins * 24 hours *  7 days)
stream_tweets(
c("ewarren,BernieSanders, realDonaldTrump, BetoORourke"),
timeout = 60 * 10,
file_name = "elec20.json",
parse = FALSE
)
df_o <- parse_stream("oloni.json")
#df_o <- stream_in(file(oloni))
# view the first 6 usernames
head(df_o$screen_name)
# create new df with the tweet text & usernames
oloni_tweet <- data.frame(date_time = df_o$created_at,
username = df_o$screen_name,
tweet_text = df_o$text)
head(oloni_tweet)
# view the first 6 usernames
head(df_e$screen_name)
# create new df with the tweet text & usernames
elec20_tweet <- data.frame(date_time = df_e$created_at,
username = df_e$screen_name,
tweet_text = df_e$text)
head(elec20_tweet)
#format = "%Y-%m-%d %H:%M:%s"
# flood start date sept 13 - 24 (end of incident)
start_date <- as.POSIXct('2013-09-13 00:00:00')
end_date <- as.POSIXct('2013-09-24 00:00:00')
# cleanup
oloni_clean <- oloni_tweet %>%
mutate(date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>%
filter(date_time >= start_date & date_time <= end_date )
min(oloni_clean$date_time)
max(oloni_clean$date_time)
# cleanup
elec20_clean <- elec20_tweet %>%
mutate(date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>%
filter(date_time >= start_date & date_time <= end_date )
min(elec20_clean$date_time)
max(elec20_clean$date_time)
colnames(df_e)
df_o$user_location_on_twitter_bio <- NA
#df_o$profile_image <- NA
df_e$user_location_on_twitter_bio <- NA
for (user in df_o$screen_name[1:200]){
print(c("finding the profile for:",user))
Sys.sleep(3) #build in a sleeper to prevent Twitter API rate limit error.
try(df_o[df_o$screen_name==user,]$user_location_on_twitter_bio <- getUser(user)$location)
try(df_o[df_o$screen_name==user,]$profile_image <- getUser(user)$profileImageUrl)
}
for (user in df_e$screen_name[1:200]){
print(c("finding the profile for:",user))
Sys.sleep(3) # build in a sleeper to prevent Twitter API rate limit error.
try(df_e[df_e$screen_name==user,]$user_location_on_twitter_bio <- getUser(user)$location)
}
df_o$lat <-NA
df_o$lng <-NA
source("https://raw.githubusercontent.com/LucasPuente/geocoding/master/geocode_helpers.R")
source("https://raw.githubusercontent.com/LucasPuente/geocoding/master/modified_geocode.R")
geocode_apply<-function(x){
geocode(x, source = "google", output = "all", api_key="AIzaSyBthydCw59313Cor8fo50EPTaydisFbEnI")
}
df_e$lat <-NA
df_e$lng <-NA
source("https://raw.githubusercontent.com/LucasPuente/geocoding/master/geocode_helpers.R")
source("https://raw.githubusercontent.com/LucasPuente/geocoding/master/modified_geocode.R")
geocode_apply<-function(x){
geocode(x, source = "google", output = "all", api_key="AIzaSyAEAmo2ItN3ydmmHtdkUCJv0b-XsSLnwXQ")
}
oloni_withgeo <- df_o[df_o$user_location_on_twitter_bio != "" & !is.na(df_o$user_location_on_twitter_bio),]
for (name in oloni_withgeo$user_location_on_twitter_bio[1:149]){ #get the coordinate data for the first 100 tweets via Google Map API.
rowid<-which(oloni_withgeo$user_location_on_twitter_bio == name)
print(paste0("getting the coordinates for:",name,", rowid is:",rowid))
Sys.sleep(1)
try(geodata <- geocode_apply(name))
if (geodata$status=="OK" & length(geodata$results)=="1") {
print(c("the lat is:",geodata$results[[1]]$geometry$location[[1]]))
print(c("the lng is:", geodata$results[[1]]$geometry$location[[2]]))
oloni_withgeo[rowid,]$lat <- geodata$results[[1]]$geometry$location[[1]]
oloni_withgeo[rowid,]$lng <- geodata$results[[1]]$geometry$location[[2]]
}else {
print ("skipping")
}
}
View(oloni_show)
df_e$lat <-NA
df_e$lng <-NA
source("https://raw.githubusercontent.com/LucasPuente/geocoding/master/geocode_helpers.R")
source("https://raw.githubusercontent.com/LucasPuente/geocoding/master/modified_geocode.R")
geocode_apply<-function(x){
geocode(x, source = "google", output = "all", api_key="AIzaSyAEAmo2ItN3ydmmHtdkUCJv0b-XsSLnwXQ")
}
elec20_withgeo <- df_e[df_e$user_location_on_twitter_bio != "" & !is.na(df_e$user_location_on_twitter_bio),]
for (name in elec20_withgeo$user_location_on_twitter_bio[1:149]){ #get the coordinate data for the first 100 tweets via Google Map API.
rowid<-which(elec20_withgeo$user_location_on_twitter_bio == name)
print(paste0("getting the coordinates for:",name,", rowid is:",rowid))
Sys.sleep(1)
try(geodata <- geocode_apply(name))
if (geodata$status=="OK" & length(geodata$results)=="1") {
print(c("the lat is:",geodata$results[[1]]$geometry$location[[1]]))
print(c("the lng is:", geodata$results[[1]]$geometry$location[[2]]))
elec20_withgeo[rowid,]$lat <- geodata$results[[1]]$geometry$location[[1]]
elec20_withgeo[rowid,]$lng <- geodata$results[[1]]$geometry$location[[2]]
}else {
print ("skipping")
}
}
#create a separate dataframe called tweets_withgeo_show. This dataframe contains only complete coordinates.
# create new df with the tweet text & usernames
elec20_tweet <- data.frame(date_time = elec20_withgeo$created_at,
username = elec20_withgeo$screen_name,
tweet_text = elec20_withgeo$text,
location = elec20_withgeo$location,
lat = elec20_withgeo$lat,
lng = elec20_withgeo$lng)
#na.omit(elec20_tweet)
map1 <- leaflet() %>% setView(lng = -98.35, lat = 39.50, zoom = 3)
map1 <- leaflet(data = elec20_tweet) %>%
addTiles() %>%
setView(lng = -98.35, lat = 39.50, zoom = 4) %>%
addCircleMarkers(lng = ~lng, lat = ~lat, popup = ~ as.character(location)) %>%
addProviderTiles("CartoDB.Positron") %>%
addCircleMarkers(
stroke = FALSE, fillOpacity = 0.5
)
#create a separate dataframe called tweets_withgeo_show. This dataframe contains only complete coordinates.
# create new df with the tweet text & usernames
elec20_tweet <- data.frame(date_time = elec20_withgeo$created_at,
username = elec20_withgeo$screen_name,
tweet_text = elec20_withgeo$text,
location = elec20_withgeo$location,
lat = elec20_withgeo$lat,
lng = elec20_withgeo$lng)
na.omit(elec20_tweet)
map1 <- leaflet() %>% setView(lng = -98.35, lat = 39.50, zoom = 3)
map1 <- leaflet(data = elec20_tweet) %>%
addTiles() %>%
setView(lng = -98.35, lat = 39.50, zoom = 4) %>%
addCircleMarkers(lng = ~lng, lat = ~lat, popup = ~ as.character(location)) %>%
addProviderTiles("CartoDB.Positron") %>%
addCircleMarkers(
stroke = FALSE, fillOpacity = 0.5
)
#create a separate dataframe called tweets_withgeo_show. This dataframe contains only complete coordinates.
# create new df with the tweet text & usernames
elec20_tweet <- data.frame(date_time = elec20_withgeo$created_at,
username = elec20_withgeo$screen_name,
tweet_text = elec20_withgeo$text,
location = elec20_withgeo$location,
lat = elec20_withgeo$lat,
lng = elec20_withgeo$lng)
elec20_tweet <- na.omit(elec20_tweet)
map1 <- leaflet() %>% setView(lng = -98.35, lat = 39.50, zoom = 3)
map1 <- leaflet(data = elec20_tweet) %>%
addTiles() %>%
setView(lng = -98.35, lat = 39.50, zoom = 4) %>%
addCircleMarkers(lng = ~lng, lat = ~lat, popup = ~ as.character(location)) %>%
addProviderTiles("CartoDB.Positron") %>%
addCircleMarkers(
stroke = FALSE, fillOpacity = 0.5
)
#create a separate dataframe called tweets_withgeo_show. This dataframe contains only complete coordinates.
# create new df with the tweet text & usernames
elec20_tweet <- data.frame(date_time = elec20_withgeo$created_at,
username = elec20_withgeo$screen_name,
tweet_text = elec20_withgeo$text,
location = elec20_withgeo$location,
lat = elec20_withgeo$lat,
lng = elec20_withgeo$lng)
elec20_tweet <- na.omit(elec20_tweet)
map1 <- leaflet() %>% setView(lng = -98.35, lat = 39.50, zoom = 3)
map1 <- leaflet(data = elec20_tweet) %>%
addTiles() %>%
setView(lng = -98.35, lat = 39.50, zoom = 4) %>%
addCircleMarkers(lng = ~lng, lat = ~lat, popup = ~ as.character(location)) %>%
addProviderTiles("CartoDB.Positron") %>%
addCircleMarkers(
stroke = FALSE, fillOpacity = 0.5
)
map1
usericon <- makeIcon(
iconUrl = elec20_withgeo_show$profile_image,
iconWidth = 15, iconHeight = 15
)
map2 <- leaflet(data = elec20_withgeo_show) %>%
addTiles() %>%
setView(lng = -98.35, lat = 39.50, zoom = 4) %>%
addMarkers(lng = ~lng, lat = ~lat, popup = ~ as.character(user_location_on_twitter_bio),icon = usericon,data = oloni_withgeo_show) %>%
addProviderTiles(providers$Esri.NatGeoWorldMap)
map2
world_basemap <- ggplot() +
borders("world", colour = "gray85", fill = "gray80")
world_basemap
# create basemap of the globe
world_basemap <- ggplot() +
borders("world", colour = "gray85", fill = "gray80") +
theme_map()
world_basemap
head(elec20_show)
head(elec20_tweet)
# remove na values
elec20_locations <- elec20_tweet %>%
na.omit()
head(elec20_locations)
world_basemap +
geom_point(data = elec20_tweet, aes(x = lng, y = lat),
colour = 'purple', alpha = .5) +
scale_size_continuous(range = c(1, 8),
breaks = c(250, 500, 750, 1000)) +
labs(title = "Tweet Locations For USA Election 2020 Thread Explosion")
site_locations <- leaflet(elec20_tweet) %>%
addTiles() %>%
addCircleMarkers(lng = ~lng, lat = ~lat, popup = ~tweet_text,
radius = 3, stroke = FALSE)
site_locations
site_locations_base <- leaflet(elec20_tweet) %>%
addProviderTiles("CartoDB.Positron") %>%
addCircleMarkers(lng = ~lng, lat = ~lat, popup = ~tweet_text,
radius = 3, stroke = FALSE)
site_locations_base
library(gapminder)
library(gganimate)
library(gifski)
## standard ggplot2
ggplot(gapminder, aes(x = lng, y = lat, size = pop, colour = country)) +
geom_point(alpha = 0.7, show.legend = FALSE) +
scale_colour_manual(values = country_colors) +
scale_size(range = c(2, 12)) +
scale_x_log10() +
# Here comes the gganimate specific bits
labs(title = 'Tweet Locations For Oloni Thread Explosion', x = 'Longitude', y = 'Latitude') +
transition_time(minute) +
ease_aes('linear')
