

# Note if packages get masked, install conflicting or use packagename::function()
```{r}
# load twitter library - the rtweet library is recommended now over twitteR
#install.packages("rtweet")
#install.packages("tidytext")
library(rtweet)
# plotting and pipes - tidyverse!
library(ggplot2)
library(dplyr)
# text mining library
library(tidytext)
# plotting packages
library(twitteR)
library(widyr)
library(tm)
library(igraph)
library(ggraph)
# json support
library(rjson)
library(jsonlite)
library(RCurl)
library(data.table)
library(leaflet)
library(RJSONIO)
library(gganimate)
library(lubridate)
library(maps)
library(ggthemes)
options(stringsAsFactors = FALSE)
```


# pass a suite of keys to the API and create a token that authenticates access to tweets! Note that the authentication process below will open a window in your browser.
```{r}
#twitter API lib in R

## install dev version of rtweet from github
#devtools::install_github("mkearney/rtweet")

## web browser method: create token and save it as an environment variable
#create_token(
#  app = "TWRAP_test",
#  consumer_key = "RkAMdcjdqcjaGTNJtFGvQLWQT",
#  consumer_secret = "hgPm0HjCzjPBi3Bs50FpM3c2latGspNZxh6156Ct5dJhFdWtqE")

#credentials got from the twitter API page, at the twitter developers site.
## access token method: create token and save it as an environment variable
twitter_token <- create_token(
  app = "TWRAP_test",
  consumer_key <- "RkAMdcjdqcjaGTNJtFGvQLWQT",
  consumer_secret<- "hgPm0HjCzjPBi3Bs50FpM3c2latGspNZxh6156Ct5dJhFdWtqE",
  access_token <- "347721539-lRLGqB52mB9sPo2l3rUoSzvxsy5G9I4lNZXbQatx",
  access_secret <- "zsyJQ21zTF0cgn0MeKS1L6e8LLQk5Le39J1ZSKuDRWJYS")



#set up authenticate
# choose yes for auto authentication or  no otherwise
setup_twitter_oauth(consumer_key ,consumer_secret,access_token ,access_secret)
```



```{r}
# post a tweet from R
post_tweet("Back at it with R in my #rstats #earthanalytics session! @EarthLabCU,
           Looking toward mapping and visualising animated tweets. 
           This should be fun. Tweeting from R is FUN. 
           I need to figure out how to add smileys from R")
```



```{r}
rstats_tweets <- search_tweets(q = "#NigeriaDecides",
                               n = 500)
# view the first 3 rows of the dataframe
head(rstats_tweets, n = 3)
```



```{r}
# find recent tweets with #rstats but ignore retweets
rstats_tweets <- search_tweets("#rstats", n = 500,
                             include_rts = FALSE)
# view top 2 rows of data
head(rstats_tweets, n = 2)
```



```{r}
# view column with screen names - top 6
head(rstats_tweets$screen_name)
```



```{r}
unique(rstats_tweets$screen_name)
```



```{r}
# what users are tweeting with #rstats
users <- search_users("#rstats",
                      n = 500)
# just view the first 2 users - the data frame is large!
head(users, n = 2)
```



```{r}
# how many locations are represented
length(unique(users$location))
## [1] 304

users %>%
  ggplot(aes(location)) +
  geom_bar() + coord_flip() +
      labs(x = "Count",
      y = "Location",
      title = "Twitter users - unique locations ")
```


### Let’s sort by count and just plot the top locations. To do this you use top_n(). Note that in this case you are grouping your data by user. Thus top_n() will return locations with atleast 15 users associated with it.
```{r}
users %>%
  count(location, sort = TRUE) %>%
  mutate(location = reorder(location, n)) %>%
  top_n(20) %>%
  ggplot(aes(x = location, y = n)) +
  geom_col() +
  coord_flip() +
      labs(x = "Count",
      y = "Location",
      title = "Where Twitter users are from - unique locations")
```


### It looks like you have some NA or no data values in your list. Let’s remove those with na.omit()
```{r}
users %>%
  count(location, sort = TRUE) %>%
  mutate(location = reorder(location,n)) %>%
  na.omit() %>%
  top_n(20) %>%
  ggplot(aes(x = location,y = n)) +
  geom_col() +
  coord_flip() +
      labs(x = "Location",
      y = "Count",
      title = "Twitter users - unique locations ")
```



```{r}
colnames(users)
```



```{r}

```


# Looking at climate data
```{r}
climate_tweets <- search_tweets(q = "#climatechange", n = 10000,
                                      lang = "en",
                                      include_rts = FALSE)
```



```{r}
# check data to see if there are emojis
head(climate_tweets$text)
```


# Data Clean-Up
```{r}
# remove urls tidyverse is failing here for some reason
# climate_tweets %>%
#  mutate_at(c("stripped_text"), gsub("http.*","",.))

# remove http elements manually
climate_tweets$stripped_text <- gsub("http.*","",  climate_tweets$text)
climate_tweets$stripped_text <- gsub("https.*","", climate_tweets$stripped_text)
```


# trying to create a list of unique words in tweets, words with capitalization will be different from words that are all lowercase
```{r}
# note the words that are recognized as unique by R
a_list_of_words <- c("Dog", "dog", "dog", "cat", "cat", ",")
unique(a_list_of_words)
```


# tidytext::unnest_tokens() function in the tidytext package magically clean up your text. tidytext::unnest_tokens() would convert text to lowercase, remove punctuation and addition of unique id associated with the tweet
```{r}
# remove punctuation, convert to lowercase, add id for each tweet!
climate_tweets_clean <- climate_tweets %>%
  dplyr::select(stripped_text) %>%
  unnest_tokens(word, stripped_text)
```



```{r}
# plot the top 15 words -- notice any issues?
climate_tweets_clean %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in tweets")
```


# Removing stop_words
```{r}
# load list of stop words - from the tidytext package
data("stop_words")
# view first 6 words
head(stop_words)


nrow(climate_tweets_clean)


# remove stop words from your list of words
cleaned_tweet_words <- climate_tweets_clean %>%
  anti_join(stop_words)

# there should be fewer words now
nrow(cleaned_tweet_words)
```



```{r}
# plot the top 15 words -- notice any issues?
cleaned_tweet_words %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(y = "Count",
      x = "Unique words",
      title = "Count of unique words found in tweets",
      subtitle = "Stop words removed from the list")
```


# Explore Networks of Words
```{r}
library(devtools)
#install_github("dgrtwo/widyr")
library(widyr)

# remove punctuation, convert to lowercase, add id for each tweet!
climate_tweets_paired_words <- climate_tweets %>%
  dplyr::select(stripped_text) %>%
  unnest_tokens(paired_words, stripped_text, token = "ngrams", n = 2)

climate_tweets_paired_words %>%
  count(paired_words, sort = TRUE)
```



```{r}
#library(tidyr)
climate_tweets_separated_words <- climate_tweets_paired_words %>%
  tidyr::separate(paired_words, c("word1", "word2"), sep = " ")

climate_tweets_filtered <- climate_tweets_separated_words %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
climate_words_counts <- climate_tweets_filtered %>%
  count(word1, word2, sort = TRUE)

head(climate_words_counts)
```


# plot climate change word network
```{r}
# plot climate change word network
climate_words_counts %>%
        filter(n >= 24) %>%
        graph_from_data_frame() %>%
        ggraph(layout = "fr") +
        geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
        geom_node_point(color = "darkslategray4", size = 3) +
        geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
        labs(title = "Word Network: Tweets using the hashtag - Climate Change",
             subtitle = "Text mining twitter data ",
             x = "", y = "")
```



```{r}
# create file path
json_file <- "/Users/lade/Documents/Tosin_R_root/boulder_flood_geolocated_tweets.json"
# import json file line by line to avoid syntax errors
# this takes a few seconds
boulder_flood_tweets <- stream_in(file(json_file))
```



```{r}

```



```{r}

```



```{r}
# Base URL path
base_url = "https://data.colorado.gov/resource/tv8u-hswn.json?"
full_url = paste0(base_url, "county=Boulder",
             "&$where=age between 20 and 40",
             "&$select=year,age,femalepopulation")
```



```{r}
full_url
```



```{r}
# encode the URL with characters for each space.
full_url <- URLencode(full_url)
full_url
```



```{r}
#Convert JSON to data frame
pop_proj_data_df <- fromJSON(getURL(full_url))
head(pop_proj_data_df, n = 2)

typeof(pop_proj_data_df)
```



```{r}
str(pop_proj_data_df)
```



```{r}
# turn columns to numeric and remove NA values
pop_proj_data_df <- pop_proj_data_df %>%
 mutate_at(c( "age", "year", "femalepopulation"), as.numeric)

str(pop_proj_data_df)
```



```{r}
# plot the data
ggplot(pop_proj_data_df, aes(x = year, y = femalepopulation,
 group = factor(age), color = age)) + geom_line() +
     labs(x = "Year",
          y = "Female Population - Age 20-40",
          title = "Projected Female Population",
          subtitle = "Boulder, CO: 1990 - 2040")
```



```{r}

```


###### oloni threads
```{r}
#Get the location of @UMassPoll. To run the following code, you must first set up Twitter API. 
user<-getUser("Oloni")
user$location
```



```{r}
## stream london tweets for a week (60 secs x 60 mins * 24 hours *  7 days)
stream_tweets(
  c("oloni,Simplyoloni"),
  timeout = 60 * 10,
  file_name = "oloni.json",
  parse = FALSE
)
```



```{r}
## stream london tweets for a week (60 secs x 60 mins * 24 hours *  7 days)
stream_tweets(
  c("ewarren,BernieSanders, realDonaldTrump, BetoORourke"),
  timeout = 60 * 10,
  file_name = "elec20.json",
  parse = FALSE
)
```



```{r}
df_o <- parse_stream("oloni.json")
#df_o <- stream_in(file(oloni))
```



```{r}
df_e <- parse_stream("elec20.json")
#df_o <- stream_in(file(oloni))
```



```{r}
# view the first 6 usernames
head(df_o$screen_name)

# create new df with the tweet text & usernames
oloni_tweet <- data.frame(date_time = df_o$created_at,
                         username = df_o$screen_name,
                         tweet_text = df_o$text)
head(oloni_tweet)
```



```{r}
# view the first 6 usernames
head(df_e$screen_name)

# create new df with the tweet text & usernames
elec20_tweet <- data.frame(date_time = df_e$created_at,
                         username = df_e$screen_name,
                         tweet_text = df_e$text)
head(elec20_tweet)
```


# filter the data to just that time period. To do this, you need to: convert the date column to a R date / time field and filter by the dates when the flood occured.
```{r}
#format = "%Y-%m-%d %H:%M:%s"
# flood start date sept 13 - 24 (end of incident)
start_date <- as.POSIXct('2013-09-13 00:00:00')
end_date <- as.POSIXct('2013-09-24 00:00:00')

# cleanup
oloni_clean <- oloni_tweet %>%
  mutate(date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>% 
  filter(date_time >= start_date & date_time <= end_date )

min(oloni_clean$date_time)

max(oloni_clean$date_time)

```



```{r}
# cleanup
elec20_clean <- elec20_tweet %>%
  mutate(date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>% 
  filter(date_time >= start_date & date_time <= end_date)

min(elec20_clean$date_time)

max(elec20_clean$date_time)
```



```{r}
colnames(df_e)
```


# Create two new columns; one for storing the location information disclosed on a user’s Twitter bio, and the other column for storing the URL to a user’s profile image.
```{r}
df_o$user_location_on_twitter_bio <- NA
#df_o$profile_image <- NA
```



```{r}
df_e$user_location_on_twitter_bio <- NA
```


# loop over each tweet, finding who sent it and the location information from the user’s Twitter bio
```{r}
for (user in df_o$screen_name[1:200]){  
  print(c("finding the profile for:",user))
  Sys.sleep(3) #build in a sleeper to prevent Twitter API rate limit error. 
  try(df_o[df_o$screen_name==user,]$user_location_on_twitter_bio <- getUser(user)$location)
  try(df_o[df_o$screen_name==user,]$profile_image <- getUser(user)$profileImageUrl)
}
```



```{r}
for (user in df_e$screen_name[1:200]){  
  print(c("finding the profile for:",user))
  Sys.sleep(3) # build in a sleeper to prevent Twitter API rate limit error.
  try(df_e[df_e$screen_name==user,]$user_location_on_twitter_bio <- getUser(user)$location)
}
```


# create a function for getting coordinates from Google Map API.We use the code published by Lucas Puente (http://lucaspuente.github.io/notes/2016/04/05/Mapping-Twitter-Followers)
```{r}
df_o$lat <-NA
df_o$lng <-NA

source("https://raw.githubusercontent.com/LucasPuente/geocoding/master/geocode_helpers.R")
source("https://raw.githubusercontent.com/LucasPuente/geocoding/master/modified_geocode.R")
geocode_apply<-function(x){
  geocode(x, source = "google", output = "all", api_key="AIzaSyBthydCw59313Cor8fo50EPTaydisFbEnI")
}
```



```{r}
df_e$lat <-NA
df_e$lng <-NA

source("https://raw.githubusercontent.com/LucasPuente/geocoding/master/geocode_helpers.R")
source("https://raw.githubusercontent.com/LucasPuente/geocoding/master/modified_geocode.R")
geocode_apply<-function(x){
  geocode(x, source = "google", output = "all", api_key="AIzaSyAEAmo2ItN3ydmmHtdkUCJv0b-XsSLnwXQ")
}
```



# We will create a separate dataframe ONLY for tweets containing geo-info. This dataframe is named tweets_withgeo. We will then loop over the user_location_on_twitter_bio column in tweets_withgeo and find corresponding coordinates for each location.
```{r}
oloni_withgeo <- df_o[df_o$user_location_on_twitter_bio != "" & !is.na(df_o$user_location_on_twitter_bio),]

for (name in oloni_withgeo$user_location_on_twitter_bio[1:149]){ #get the coordinate data for the first 100 tweets via Google Map API.
  rowid<-which(oloni_withgeo$user_location_on_twitter_bio == name)
  print(paste0("getting the coordinates for:",name,", rowid is:",rowid))
  Sys.sleep(1)
  try(geodata <- geocode_apply(name))
  
    if (geodata$status=="OK" & length(geodata$results)=="1") {
    print(c("the lat is:",geodata$results[[1]]$geometry$location[[1]]))
    print(c("the lng is:", geodata$results[[1]]$geometry$location[[2]]))
    oloni_withgeo[rowid,]$lat <- geodata$results[[1]]$geometry$location[[1]]
    oloni_withgeo[rowid,]$lng <- geodata$results[[1]]$geometry$location[[2]]
  }else {
    print ("skipping")
  }
}
```



```{reval=FALSE}
geocode_results <- sapply(elec20_withgeo$location, geocode_apply, simplify = F)

colnames(elec20_withgeo)

elec20_withgeo$lat
```



```{r}
elec20_withgeo <- df_e[df_e$user_location_on_twitter_bio != "" & !is.na(df_e$user_location_on_twitter_bio),]

for (name in elec20_withgeo$user_location_on_twitter_bio[1:149]){ #get the coordinate data for the first 100 tweets via Google Map API.
  rowid<-which(elec20_withgeo$user_location_on_twitter_bio == name)
  print(paste0("getting the coordinates for:",name,", rowid is:",rowid))
  Sys.sleep(1)
  try(geodata <- geocode_apply(name))
  
    if (geodata$status=="OK" & length(geodata$results)=="1") {
    print(c("the lat is:",geodata$results[[1]]$geometry$location[[1]]))
    print(c("the lng is:", geodata$results[[1]]$geometry$location[[2]]))
    elec20_withgeo[rowid,]$lat <- geodata$results[[1]]$geometry$location[[1]]
    elec20_withgeo[rowid,]$lng <- geodata$results[[1]]$geometry$location[[2]]
  }else {
    print ("skipping")
  }
} 
```



```{r}
#create a separate dataframe called tweets_withgeo_show. This dataframe contains only complete coordinates. 
oloni_withgeo_show <- oloni_withgeo[!is.na(oloni_withgeo$lat),c("lat","lng", "user_location_on_twitter_bio", "text")]

# create new df with the tweet text & usernames
oloni_show <- data.frame(date_time = oloni_withgeo$created_at,
                         username = oloni_withgeo$screen_name,
                         tweet_text = oloni_withgeo$text,
                         location = oloni_withgeo$location,
                         lat = oloni_withgeo$lat,
                         lng = oloni_withgeo$lng)
map_o <- leaflet() %>% setView(lng = -98.35, lat = 39.50, zoom = 3)
map_o <- leaflet(data = oloni_withgeo_show) %>% 
  addTiles() %>%
  setView(lng = -98.35, lat = 39.50, zoom = 4) %>% 
  addCircleMarkers(lng = ~lng, lat = ~lat, popup = ~ as.character(user_location_on_twitter_bio)) %>% 
  addProviderTiles("CartoDB.Positron") %>%
  addCircleMarkers(
    stroke = FALSE, fillOpacity = 0.5
  )
```



```{r}
#create a separate dataframe called tweets_withgeo_show. This dataframe contains only complete coordinates. 

# create new df with the tweet text & usernames
elec20_tweet <- data.frame(date_time = elec20_withgeo$created_at,
                         username = elec20_withgeo$screen_name,
                         tweet_text = elec20_withgeo$text,
                         location = elec20_withgeo$location,
                         lat = elec20_withgeo$lat,
                         lng = elec20_withgeo$lng)

elec20_tweet <- na.omit(elec20_tweet)

map1 <- leaflet() %>% setView(lng = -98.35, lat = 39.50, zoom = 3)
map1 <- leaflet(data = elec20_tweet) %>% 
  addTiles() %>%
  setView(lng = -98.35, lat = 39.50, zoom = 4) %>% 
  addCircleMarkers(lng = ~lng, lat = ~lat, popup = ~ as.character(location)) %>% 
  addProviderTiles("CartoDB.Positron") %>%
  addCircleMarkers(
    stroke = FALSE, fillOpacity = 0.5
  )
map1
```



```{r}
usericon <- makeIcon(
  iconUrl = elec20_withgeo_show$profile_image,
  iconWidth = 15, iconHeight = 15
)
map2 <- leaflet(data = elec20_withgeo_show) %>% 
  addTiles() %>%
  setView(lng = -98.35, lat = 39.50, zoom = 4) %>% 
  addMarkers(lng = ~lng, lat = ~lat, popup = ~ as.character(user_location_on_twitter_bio),icon = usericon,data = oloni_withgeo_show) %>% 
  addProviderTiles(providers$Esri.NatGeoWorldMap) 
map2
```


# create basemap of the globe
```{r}
world_basemap <- ggplot() +
  borders("world", colour = "gray85", fill = "gray80")

world_basemap
```


# Below you can see how the theme_map() function cleans up the look of your map
```{r}
# create basemap of the globe
world_basemap <- ggplot() +
  borders("world", colour = "gray85", fill = "gray80") +
  theme_map()

world_basemap
```



```{r}
head(elec20_tweet)
```



```{r}
# remove na values
elec20_locations <- elec20_tweet %>%
  na.omit()

head(elec20_locations)
```



```{r}
world_basemap +
  geom_point(data = elec20_tweet, aes(x = lng, y = lat),
             colour = 'purple', alpha = .5) +
  scale_size_continuous(range = c(1, 8),
                        breaks = c(250, 500, 750, 1000)) +
  labs(title = "Tweet Locations For USA Election 2020 Thread Explosion")
```


# plot points on top of a leaflet basemap
```{r}
site_locations <- leaflet(elec20_tweet) %>%
  addTiles() %>%
  addCircleMarkers(lng = ~lng, lat = ~lat, popup = ~tweet_text,
                   radius = 3, stroke = FALSE)

site_locations

```


# plot points on top of a leaflet basemap
```{r}
site_locations_base <- leaflet(elec20_tweet) %>%
  addProviderTiles("CartoDB.Positron") %>%
  addCircleMarkers(lng = ~lng, lat = ~lat, popup = ~tweet_text,
                   radius = 3, stroke = FALSE)

site_locations_base
```



```{r}
# created animated gif file
#gganimate(grouped_tweet_map)

# save the animation to a new file
gganimate_save(grouped_tweet_map,
               filename = "~/oloni_tweets.gif",
               fps = 1, loop = 0,
               width = 1280,
               height = 1024)
```



```{r}
library(gapminder)
library(gganimate)
library(gifski)

## standard ggplot2
ggplot(gapminder, aes(x = lng, y = lat, size = pop, colour = country)) +
  geom_point(alpha = 0.7, show.legend = FALSE) +
  scale_colour_manual(values = country_colors) +
  scale_size(range = c(2, 12)) +
  scale_x_log10() +
  # Here comes the gganimate specific bits
  labs(title = 'Tweet Locations For Oloni Thread Explosion', x = 'Longitude', y = 'Latitude') +
  transition_time(minute) +
  ease_aes('linear')
```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```
